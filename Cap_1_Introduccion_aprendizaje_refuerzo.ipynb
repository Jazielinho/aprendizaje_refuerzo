{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cap_1_Introduccion_aprendizaje_refuerzo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPgMM0nhgKLwxjdOJEJNEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jazielinho/aprendizaje_refuerzo/blob/main/Cap_1_Introduccion_aprendizaje_refuerzo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Dzr1PRn6TL"
      },
      "source": [
        "#1.1 Contexto\n",
        "\n",
        "##Inteligencia Artificial\n",
        "Es una disciplina que se ocupa de crear programas informáticos que muestren una \"inteligencia\" similar a la humana.\n",
        "\n",
        "##Machine Learning\n",
        "*Aprendizaje automático*. Subcampo de la inteligencia artificial que proporciona a los computadores la capacidad de aprender sin ser explícitamente programados, es decir, sin que estos necesiten que el programador indique las reglas que deben seguir para lograr su tarea, sino que la hacen automáticamente.\n",
        "\n",
        "###Aprendizaje Supervisado\n",
        "Los datos que usamos para el entrenamiento incluyen la solución deseada, llamada etiqueta (**label**). En este caso el aprendizaje radica en aprender un modelo (o función) que mapea una entrada a una salida. El modelo, una vez entrenado, predecirá las etiquetas para datos de entrada no vistos anteriormente.\n",
        "\n",
        "###Aprendizaje no Supervisado\n",
        "Los datos de entrenamiento no incluyen las etiquetas, y es el algoritmo el que intentará clasificar la información por sí mismo.\n",
        "\n",
        "###Aprendizaje por refuerzo\n",
        "El modelo se implementa en forma de un agente que deberá explorar un espacio desconocido y determinar las acciones a llevar a cabo mediante \"prueba y error\". Aprenderá por sí mismo gracias a las recompensas y penalizaciones que obtiene de sus acciones. El agente debe actuar y crear la mejor estrategia posible para obtener la mayor recompensa en tiempo y forma.\n",
        "\n",
        "###Deep Learning\n",
        "*Aprendizaje profundo*. Se basan en redes neuronales artificiales, cuyas estructuras algorítmicas permiten que modelos compuestos por múltiples capas de procesamiento aprendan representaciones de datos con varios niveles de abstracción."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*YIETknPBlQQBF40DJxL8QA.png)"
      ],
      "metadata": {
        "id": "fzlTfYdpg6oh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOUK-W59qdP_"
      },
      "source": [
        "###Aprendizaje por refuerzo profundo\n",
        "Se adoptan soluciones que resultan de la utilización simultánea de técnicas de aprendizaje por refuerzo (Reinforcement Learning) y técnicas de aprendizaje profundo (Deep Learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpSPsT0opbJf"
      },
      "source": [
        "#1.2 Aprendizaje por Refuerzo\n",
        "\n",
        "##Aprender interactuando\n",
        "*Aprendizaje dirigido* a objetivos a partir de la interacción. A la entidad que aprende no se le dice qué acciones realizar, sino que debe descubrir por sí misma — mediante \"prueba y error\" — qué acciones producen la mayor *recompensa*. Objetivo: maximizar la *recompensa*.\n",
        "\n",
        "Estas acciones afectan la recompensa inmediata y futura (\"recompensas retrasadas\"). Las acciones actuales determinarán situaciones futuras (como ocurre en la vida real). \n",
        "\n",
        "Dos características distintivas del aprendizaje por refuerzo: búsqueda por \"prueba y error\" y recompensa retrasada (delayed *reward* en inglés)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzc9D10Stjjb"
      },
      "source": [
        "##Piezas básicas del Aprendizaje por Refuerzo\n",
        "\n",
        "###Agente y Entorno\n",
        "\n",
        "####Agente\n",
        "*agent* representa la \"solución\". Su única opción es tomar decisiones (realizar acciones), para resolver problemas de toma de decisiones bajo incertidumbre.\n",
        "\n",
        "####Entorno\n",
        "*enviroment* representación de un \"problema\", todo lo que viene después de la decisión del agente. El entorno responde con la consecuencia de las acciones, observables o estados, y recompensa.\n",
        "\n",
        "Estos dos componentes interactúan continuamente, el agente intenta influir en el entorno y el entorno reacciona a las acciones del agente. La reacción del agente tiene dos situaciones:\n",
        "\n",
        "* El agente conoce el modelo del entorno. Aprendizaje por refuerzo basado en modelo, o *model-based*. Solución: programación dinámica.\n",
        "\n",
        "* El agente no conoce el modelo del entorno, debe tomar decisiones con información incompleta. El agente puede intentar aprender el modelo como parte del algoritmo. Aprendizaje por refuerzo sin modelo, o *model-free*.\n",
        "\n",
        "###Estado\n",
        "El entorno está representado por un espacio de estados, que es un conjunto de variables relacionadas con el problema. Un estado (state) es una instancia del espacio de estados, un conjunto de valores que toman las variables.\n",
        "\n",
        "###Acción y función de transición\n",
        "En cada estado, el entorno pone a disposición un *conjunto de acciones*, el agente elegirá una *acción*. El agente afecta en el entorno y el entorno puede cambiar de estado en respuesta.\n",
        "\n",
        "Función responsable del mapeo: *función de transición* (transition function) o probabilidades de transición (transition probabilities).\n",
        "\n",
        "Model-free: el agente debe estimar esta función de transición.\n",
        "\n",
        "###Recompensa\n",
        "Por cada acción del agente el entorno proporciona señal de *recompensa*. La función responsable de el mapeo de cada acción recompensa se llama *función de recompensa* (reward function).\n",
        "\n",
        "El objetivo del agente es maximizar la recompensa.\n",
        "\n",
        "La función de recompensa es una de las partes más complejas en la modelización de un problema.\n",
        "\n",
        "###Ciclo de aprendizaje por refuerzo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*M98RSO-cjMXTB4Ua5ixWwA.png)"
      ],
      "metadata": {
        "id": "LOWLR5LThDBN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcW42CSsCILg"
      },
      "source": [
        "1. El agente *observa el entorno* a partir del estado y la recompensa de su acción anterior.\n",
        "2. El agente usa el estado y recompensa para *decidir* la siguiente acción.\n",
        "3. El agente realiza una *acción* sobre el entorno en un intento de controlarlo de manera favorable para él.\n",
        "4. el entorno *reacciona* a la acción y su estado interno cambia como consecuencia de la acción del agente.\n",
        "\n",
        "Se repite el bucle.\n",
        "\n",
        "*time step*, o ciclo: marca una interacción entre el agente y el entorno en el tiempo. Llamado también *iteración*. En cada time step se realiza el bucle.\n",
        "\n",
        "*experiencia*: conjunto del estado, acción recompensa y el nuevo estado en cada *time step*.\n",
        "\n",
        "###Episodio\n",
        "La tarea que el agente está tratando de resolver pueden tener un final natural (*tareas episódicas*, ejemplo, ajedrez) o no (*tareas continuas*, ejemplo, robot aprendiendo a mover). \n",
        "* episodio: secuencia de time steps desde el principio hasta el final de una tarea episódica\n",
        "* trayectoria: cuando consideramos una parte de su \"episodio infinito\" (tarea continua).\n",
        "\n",
        "###Retorno\n",
        "*return*, suma de las recompensas recolectadas en un solo *episodio*. Esta medida es la que guía en el aprendizaje de los agentes.\n",
        "\n",
        "* recompensa retrasada (delayed reward): la recompensa se revela al final de un episodio. Ejemplo, 3 en raya.\n",
        "\n",
        "###Exploración versus explotación\n",
        "un agente tiene que *explotar* (repetir acciones del pasado) para obtener la mayor recompensa posible pero, al mismo tiempo, también tiene que *explorar* (probar nuevas acciones) para poder descubrir de mejores acciones en el futuro (dilema *exploración-explotación*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/Jazielinho/aprendizaje_refuerzo/main/cap_1_imagen.png)"
      ],
      "metadata": {
        "id": "leN64ESVhiiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIev8_orJ2UQ"
      },
      "source": [
        "#1.3 Modelización de un problema\n",
        "\n",
        "##Frozen-Lake\n",
        "El entorno simula una pista de patinaje sobre hielo. Está dividida en 16 celdas (4x4), en algunas de estas celdas se ha roto el hielo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*iTc5J7JcoZq-Y_In8LiuYw.png)"
      ],
      "metadata": {
        "id": "BIbsjGamhIiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El patinador (*agente*) empieza a patinar en la posición superior izquierda.\n",
        "* Su objetivo es llegar a la posición inferior izquierda.\n",
        "* Si el agente se cae en uno de los agujeros, el *episodio* termina. La *recompensa* es 0.\n",
        "* Si el agente llega a la celda de destino, la *recompensa* es +1 (termina el *episodio*)\n",
        "* La cuadrícula es 4x4, el *espacio de estados* es 16 (del 0 al 15).\n",
        "* *espacio de acciones*: 4 posibles movimientos: izquierda (left), abajo (down), derecha (right) y arriba (up).\n",
        "* Si el agente intenta salir de la cuadrícula, rebotará en la valla transparente y volverá a quedarse en la misma celda.\n"
      ],
      "metadata": {
        "id": "t5OGlZXRP0NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*NmDW_vfs8chXxXAoYc5jRw.png)"
      ],
      "metadata": {
        "id": "DLYbRq5NhM6W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KQBWFT9MUxk"
      },
      "source": [
        "##Programación del entorno\n",
        "\n",
        "Librería usada `gym`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "lkedhYD0TP0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Usaremos el entorno Frozen-Lake'''\n",
        "\n",
        "# is_slippery=False indica que el entorno no es resbaladizo\n",
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "\n",
        "'''Establecemos el estado inicial'''\n",
        "entorno.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcCQ0MZkTXCn",
        "outputId": "95e9f09d-b9b2-4c86-831a-864c42a81fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Para ver una vista del estado del juego'''\n",
        "entorno.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A17zVJS-Tvmb",
        "outputId": "aa842442-473a-4ebb-e94e-b50a4d8d1fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* S (start): celdra de inicio.\n",
        "* F (frozen): celda congelada.\n",
        "* H (hole): celda con agujero.\n",
        "* G (goal): celda objetivo."
      ],
      "metadata": {
        "id": "GXdh1I4uT12D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Programación del agente\n",
        "\n",
        "Programación del agente torpe"
      ],
      "metadata": {
        "id": "TUOJGZReTOzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entorno no resbaladizo\n",
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "entorno.reset()\n",
        "\n",
        "esta_terminado = False\n",
        "ciclo = 0\n",
        "\n",
        "# Creamos un bucle para decidir la acción del agente\n",
        "while not esta_terminado:\n",
        "  # Devuelve una acción aleatoria del espacio de acciones posibles\n",
        "  accion = entorno.action_space.sample()\n",
        "  # El método step devuelve 4 argumentos:\n",
        "  estado, recompensa, esta_terminado, _ = entorno.step(accion)\n",
        "  # Observando los movimientos\n",
        "  entorno.render()\n",
        "  ciclo += 1\n",
        "\n",
        "print(f'Ultimo estado: {estado}')\n",
        "print(f'Recompensa: {recompensa}')\n",
        "print(f'Número de ciclos: {ciclo}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvCIB3q7UYOd",
        "outputId": "b28090bd-1ab1-4e82-fb0c-85babdd97d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Ultimo estado: 5\n",
            "Recompensa: 0.0\n",
            "Número de ciclos: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo es que nuestro agente pueda alcanzar el destino."
      ],
      "metadata": {
        "id": "Ysa2elCfWCy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QLyG8oUS9Rq"
      },
      "source": [
        "###La clase Agent (Agente)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una clase Agente, el cual tiene el método `selecciona_accion` para decidir qué acción va a realizar:"
      ],
      "metadata": {
        "id": "t5hTuwFlbDnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agente:\n",
        "  def __init__(self):\n",
        "    self.entorno_interno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "  \n",
        "  def selecciona_accion(self):\n",
        "    # Decide de manera aleatoria la acción\n",
        "    return self.entorno_interno.action_space.sample()"
      ],
      "metadata": {
        "id": "u03YhamQWNh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probando el resultado de nuestro agente torpe:"
      ],
      "metadata": {
        "id": "5MKrdAcTbSs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entorno no resbaladizo\n",
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "entorno.reset()\n",
        "\n",
        "esta_terminado = False\n",
        "ciclo = 0\n",
        "\n",
        "# Agente torpre\n",
        "agente = Agente()\n",
        "\n",
        "# Creamos un bucle para decidir la acción del agente\n",
        "while not esta_terminado:\n",
        "  # El agente genera la accion\n",
        "  accion = agente.selecciona_accion()\n",
        "  estado, recompensa, esta_terminado, _ = entorno.step(accion)\n",
        "  # Observando los movimientos\n",
        "  entorno.render()\n",
        "  ciclo += 1\n",
        "\n",
        "print(f'Ultimo estado: {estado}')\n",
        "print(f'Recompensa: {recompensa}')\n",
        "print(f'Número de ciclos: {ciclo}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbCR-W0FWizo",
        "outputId": "36d020ae-17e4-4a58-a2cd-c80329539717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Ultimo estado: 5\n",
            "Recompensa: 0.0\n",
            "Número de ciclos: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutando varias veces para estimar el porcentaje de éxito para el Agente torpe(aleatorio)"
      ],
      "metadata": {
        "id": "dCbzC1xYbkEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(agente, entorno_test):\n",
        "  # Reiniciamos el entorno\n",
        "  entorno_test.reset()\n",
        "  esta_terminado = False\n",
        "  ciclo = 0\n",
        "\n",
        "  while not esta_terminado:\n",
        "    # El agente genera la accion aleatoria\n",
        "    accion = agente.selecciona_accion()\n",
        "    # El método step devuelve 4 argumentos:\n",
        "    estado, recompensa, esta_terminado, _ = entorno.step(accion)\n",
        "    ciclo += 1\n",
        "  \n",
        "  return estado, recompensa, esta_terminado"
      ],
      "metadata": {
        "id": "SBti4lfSWyha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos al agente\n",
        "agente = Agente()\n",
        "\n",
        "# Creamos el entorno\n",
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)"
      ],
      "metadata": {
        "id": "QkoxvGG2XJe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteramos mil veces y calculamos el porcentaje de éxito del Agente\n",
        "import tqdm # Para visualizar las iteraciones\n",
        "\n",
        "resuelto = 0\n",
        "\n",
        "for episodio in tqdm.tqdm(range(1000)):\n",
        "  estado, recompensa, esta_terminado = test(agente, entorno)\n",
        "  if estado == 15:\n",
        "    resuelto += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eA-p8J7XPbn",
        "outputId": "a0143563-2fe0-4dbf-b528-b99ba01bee72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 7733.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Resuelto {resuelto} veces, porcentaje: {resuelto / 10}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNh-xp1SXa6P",
        "outputId": "b55916f9-2d63-4813-fde5-0d0ddd5622d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuelto 10 veces, porcentaje: 1.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 En qué se diferencia al aprendizaje\n",
        "\n",
        "* Aprendizaje por refuerzo vs aprendizaje supervisado: El aprendizaje supervisado necesita datos etiquetados, el aprendizaje por refuerzo interactua con un entorno y recibirá una recompensa del cuál aprenderá.\n",
        "\n",
        "* Aprendizaje por refuerzo vs aprendizaje no supervisado: En el aprendizaje no supervisado el modelo aprende de la estructura oculta de los datos, en el aprendizaje por refuerzo, el modelo aprende de las recompensas que intenta maximizar."
      ],
      "metadata": {
        "id": "VWdz-svJcMXc"
      }
    }
  ]
}