{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap_2_formalizacion_aprendizaje_refuerzo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNg7H3GNl/T6LNjHue/Ev+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jazielinho/aprendizaje_refuerzo/blob/main/cap_2_formalizacion_aprendizaje_refuerzo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Proceso de decisión de Markov"
      ],
      "metadata": {
        "id": "ROZBQYMkdQpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recapitulación de conceptos\n",
        "\n",
        "Un agente influye en el comportamiento observado de un entorno eligiendo acciones. El objetivo es elegir las acciones que maximicen la recompensa.\n",
        "\n",
        "El agente y el entorno interactúan en una secuencia de *time step* discretos.\n",
        "\n",
        "En el time step $t$:\n",
        "* $s$: estado del entorno ($s_{t})$\n",
        "* $a$: acción del agente ($a_{t})$\n",
        "\n",
        "La acción $a$ recibe en el siguiente time step $t + 1$:\n",
        "* $r'$ recompensa ($r_{t+1})$\n",
        "* $s'$: el entorno cambia al nuevo estado ($s_{t+1})$\n",
        "\n",
        "El conjunto del estado, acción, recompensa y nuevo estado se llama *experiencia*"
      ],
      "metadata": {
        "id": "SupT0jp5eEwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*x-Bj67wALCbWrtSNhb5h0A.png)"
      ],
      "metadata": {
        "id": "DwvJpv7WgjK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proceso de decisión de Markov\n",
        "O *Markov Decision Process* (MDP), marco matemático para resolver algunos de los problemas de aprendizaje por refuerzo (RL)\n",
        "\n",
        "Consiste en una tupla de 5 elementos: $<S, A, R, P, γ>$\n",
        "\n",
        "* $S$: conjunto de estados.\n",
        "* $A$: conjunto de acciones.\n",
        "* $R$: función de recompensa.\n",
        "* $P$: función de transición.\n",
        "* $\\gamma$: factor de descuento."
      ],
      "metadata": {
        "id": "WhBtMjz8iegT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Piezas de un proceso de decisión de Markov"
      ],
      "metadata": {
        "id": "rlA6aLztjBxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "entorno  = gym.make('FrozenLake-v0', is_slippery=False)"
      ],
      "metadata": {
        "id": "9THHTAWbl5Xo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estados\n",
        "\n",
        "U *observación* es una configuración única y autónoma del problema.\n",
        "\n",
        "*espacio de estado*: conjunto de todos los estados posibles, puede ser infinito."
      ],
      "metadata": {
        "id": "vvrZ_OvmmKI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Espacio de estados '''\n",
        "print(f\"Espacio de estados: {entorno.observation_space}\")"
      ],
      "metadata": {
        "id": "XX133hLWeEMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9aaa23c-e560-4b15-d5ff-2d32f065c196"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Espacio de estados: Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estados particulares, ejemplo, estado de partida o estados terminales. En Frozen Lake: *estado inicial* (0), 5 *estados terminales* (5, 7, 11, 12, 15)"
      ],
      "metadata": {
        "id": "mklxn6RSmpMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*yTHeP5HSKXCpnv9wbgGIoQ.png)"
      ],
      "metadata": {
        "id": "gmzgg1ihoODr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Propiedad de Markov*: el futuro depende solo del presente y no del pasado. El estado actual que obtenemos del entorno contiene lo que se necesita para decidir el estado futuro cuando se realiza una acción.\n",
        "\n",
        "La mayoría de los métodos de RL están diseñados para aprovechar esta suposición.\n",
        "\n",
        "Ejemplo, Frozen Lake, estado actual: 2, el agente solo puede realizar la transición a 1, 2, 3, o 6, independientemente del estado anterior."
      ],
      "metadata": {
        "id": "Yn7EDaKfoWyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acciones\n",
        "Un conjunto de acciones $A$ dependen del estado, por lo que pueden variar en cada *time step*.\n",
        "El agente elige una acción, esto influye en el entorno y el entorno cambia el estado."
      ],
      "metadata": {
        "id": "ghCD3NHCpJiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 4 acciones '''\n",
        "''' izquierda, abajo, derecha, arriba '''\n",
        "print(f\"Espacio de acciones: {entorno.action_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF3W5sU0mIhS",
        "outputId": "58c64d7e-6a53-4182-a400-1db5127d3197"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Espacio de acciones: Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Espacio de acción discreto: el espacio de acción consta de acciones discretas (ejemplo, Frozen Lake)\n",
        "* Espacio de acción continua: el espacio de acción compone de acciones continuas (ejemplo, conducción)"
      ],
      "metadata": {
        "id": "CCBfwMZJqFWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función de transición\n",
        "\n",
        "$P$, representa la probabilidad de moverse de un estado a otro. Forma parte del MDP, no es conocida por el agente, representa la dinámica de un *time step* del entorno.\n",
        "\n",
        "$P(s'|a)$ Quiere expresar que el siguiente estado $s'$ depende solo del estado anterior $s$ y la acción $a$."
      ],
      "metadata": {
        "id": "Q4BVP-Ggr0GS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recompensa\n",
        "\n",
        "$R(s, a)$ o $R(s, a, s')$ representa la recompensa que nuestro agente obtiene durante la transición de estado $s$ al estado $s'$ al realizar una acción $a$. Es un número real. Este valor dice al agente cuán bien se ha comportado.\n",
        "\n",
        "En Frozen Lake, la función de recompensa es \"+1\" cuando llega al estado final y \"0\" en cualquier otro caso.\n",
        "\n",
        "*return*: indica la recompensa acumulada."
      ],
      "metadata": {
        "id": "sF_ClyWxugY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factor de descuento\n",
        "* *tareas epísodicas*: cuando las tareas tienen un final natural. La secuencia *time step* se llama *episodio*.\n",
        "* *tareas continuas*: cuando no tienen un final natural.\n",
        "\n",
        "En el caso de tareas continuas necesitamos una forma de descontar el valor de las recompensas a lo largo del tiempo.\n",
        "\n",
        "*factor de descuento* $γ \\in [0, 1]$, ajusta la importancia de las recompensas a lo largo del tiempo.\n",
        "\n",
        "Cuanto más tarde reciba las recompensas el agente, menos atractivo será para él obtenerla."
      ],
      "metadata": {
        "id": "KeEsmvoNv7j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Entornos deterministas y estocásticos\n",
        "\n",
        "El agente conocerá el estado, acciones, recompensas (más factor de descuento), pero la función de transición por lo general será desconocida.\n",
        "\n",
        "Los estados y recompensas pueden ser variables aleatorias porque la mayoría de los entornos son estocásticos.\n",
        "\n",
        "Dos tipos: *entornos deterministas* y *entornos estocásticos*"
      ],
      "metadata": {
        "id": "TWJRfEyTwQtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entorno determinista"
      ],
      "metadata": {
        "id": "3Z6EbHeFwVVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "#is_slippery=False indica no resbaladizo. Entorno determinista\n",
        "entorno  = gym.make('FrozenLake-v0', is_slippery=False)"
      ],
      "metadata": {
        "id": "O1RMOh422fLd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La probabilidad en el *time step* $t$ del siguiente estado $s_{t+1}$, dado el estado actual $s_t$ y la acción $a_t$ es siempre 1.\n",
        "\n",
        "$p(s_{t+1}|s_t, a_t) = 1$"
      ],
      "metadata": {
        "id": "Chdp-BAC3D6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entorno.env.P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAsHBcjg3grQ",
        "outputId": "b660adf1-c635-420a-cd44-9a9a7dfebb7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de salida del diccionario:\n",
        "\n",
        "0: {0: [(1.0, 0, 0.0, False)],}\n",
        "\n",
        "Cada estado contiene un diccionario que mapea todas las acciones posibles.\n",
        "\n",
        "* Primer 0: ubicación en la matriz.\n",
        "* Segundo 0: dirección (izquierda, abajo, derecha, arriba)\n",
        "* Lista:\n",
        "  * 1.0: Probabilidad de ir al siguiente estado (igual a 1, entorno determinista)\n",
        "  * 0: El siguiente estado (nueva ubicación en la matriz)\n",
        "  * 0.0: recompensa al pasar al siguiente estado.\n",
        "  * False: si el episocio termina allí o no."
      ],
      "metadata": {
        "id": "10qONnAi4QU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*M7viRSF1GTwDV3twKWzGSw.png)"
      ],
      "metadata": {
        "id": "2Rg1d2bl88qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programando al agente para realizar un buen plan (llegar a la celda final con éxito):"
      ],
      "metadata": {
        "id": "nVNS30P_SODB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agente:\n",
        "  def __init__(self):\n",
        "    self.acciones = {'Izquierda': 0, 'Abajo': 1, 'Derecha': 2, 'Arriba': 3}\n",
        "    self.buen_plan = 2 * ['Abajo'] + ['Derecha'] + ['Abajo'] + 2 * ['Derecha']\n",
        "    self.step = 0\n",
        "  \n",
        "  def selecciona_accion(self):\n",
        "    accion = self.buen_plan[self.step]\n",
        "    self.step = (self.step + 1) % 6\n",
        "    return self.acciones[accion]\n",
        "  \n",
        "  def reiniciar(self):\n",
        "    ''' inicializamos el agente cada vez que es llamado '''\n",
        "    self.step = 0"
      ],
      "metadata": {
        "id": "HWI-64fF9EwI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "\n",
        "entorno.reset()\n",
        "entorno.render()\n",
        "esta_terminado = False\n",
        "t = 0\n",
        "\n",
        "agente = Agente()\n",
        "\n",
        "while not esta_terminado:\n",
        "  accion = agente.selecciona_accion()\n",
        "  estado, recompensa, esta_terminado, _ = entorno.step(accion)\n",
        "  entorno.render()\n",
        "  t += 1\n",
        "\n",
        "print(f\"Último estado: {estado}\")\n",
        "print(f\"Recompensa: {recompensa}\")\n",
        "print(f\"time steps: {t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN5p5QO-92c2",
        "outputId": "beaefb6d-2145-47ac-c63f-d5c21f05fc86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Último estado: 15\n",
            "Recompensa: 1.0\n",
            "time steps: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Funcion que calcula el éxito del agente '''\n",
        "\n",
        "def test(agente, entorno_test):\n",
        "  entorno_test.reset()\n",
        "  agente.reiniciar()\n",
        "\n",
        "  esta_terminado = False\n",
        "  t = 0\n",
        "\n",
        "  while not esta_terminado:\n",
        "    accion = agente.selecciona_accion()\n",
        "    estado, recompensa, esta_terminado, _ = entorno_test.step(accion)\n",
        "    t += 1\n",
        "  \n",
        "  return estado, recompensa, esta_terminado"
      ],
      "metadata": {
        "id": "DXKrR-f1TQBe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Probando la función 1000 veces para calcular la proporción de aciertos '''\n",
        "\n",
        "agente = Agente()\n",
        "entorno = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "resuelto = 0\n",
        "\n",
        "for episodio in range(1000):\n",
        "  estado, recompensa, esta_terminado = test(agente, entorno)\n",
        "  if estado == 15:\n",
        "    resuelto += 1\n",
        "\n",
        "print(f\"Resuelto: {resuelto}, {resuelto/10}% de las veces\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSIZIaw3Trl5",
        "outputId": "c01707d7-5e78-4ec1-91d7-ce0d3f760fdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuelto: 1000, 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entorno estocástico\n",
        "\n",
        "La función de transición es un poco más compleja, ya que debe indicar las diversas probabilidades de acabar en diferentes estados que tiene cada acción.\n",
        "\n",
        "Para Frozen Lake la función de transición se representa en una matriz 3D, donde cada elemento indica la probabilidad de transición de un estado de origen $s$ a un destino $s'$ dada una acción $a$"
      ],
      "metadata": {
        "id": "E6qALvcGwZ3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' La función de transición en 3D '''\n",
        "entorno = gym.make('FrozenLake-v0')\n",
        "entorno.env.P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sAiEG8pXhOM",
        "outputId": "07132b8f-a229-4dcd-d13c-7a348ac8de1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)]},\n",
              " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)]},\n",
              " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 6, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)]},\n",
              " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False)]},\n",
              " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 0, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 9, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False)]},\n",
              " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  3: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 8, 0.0, False)]},\n",
              " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 11, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 11, 0.0, True),\n",
              "   (0.3333333333333333, 6, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 11, 0.0, True),\n",
              "   (0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 13, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True)]},\n",
              " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 15, 1.0, True)],\n",
              "  2: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 15, 1.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 15, 1.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La probabilidad de ir a la celda prevista es del 33%, 66% de ir a otras celdas. Si estamos al lado de una valla transparente y nos dirigimos hacia ella, no necesariamente volveremos a la misma celda."
      ],
      "metadata": {
        "id": "fT7VYXhsaLJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Probando el Agente creado en este entorno estocástico '''\n",
        "\n",
        "agente = Agente()\n",
        "entorno = gym.make('FrozenLake-v0')\n",
        "resuelto = 0\n",
        "\n",
        "for episodio in range(1000):\n",
        "  estado, recompensa, esta_terminado = test(agente, entorno)\n",
        "  if estado == 15:\n",
        "    resuelto += 1\n",
        "\n",
        "print(f\"Resuelto: {resuelto}, {resuelto/10}% de las veces\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T46hggMUa8gH",
        "outputId": "6ed2b5b9-eedf-449a-bd3d-8a6681a8c11f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuelto: 49, 4.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si estamos en el estado 14, esta es a función de transición:"
      ],
      "metadata": {
        "id": "H1YXi_ZTbXWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entorno = gym.make('FrozenLake-v0')\n",
        "entorno.env.P[14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRwdaUecbblL",
        "outputId": "d1527bdb-406c-44a4-90b1-8ddf08bdc09b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(0.3333333333333333, 10, 0.0, False),\n",
              "  (0.3333333333333333, 13, 0.0, False),\n",
              "  (0.3333333333333333, 14, 0.0, False)],\n",
              " 1: [(0.3333333333333333, 13, 0.0, False),\n",
              "  (0.3333333333333333, 14, 0.0, False),\n",
              "  (0.3333333333333333, 15, 1.0, True)],\n",
              " 2: [(0.3333333333333333, 14, 0.0, False),\n",
              "  (0.3333333333333333, 15, 1.0, True),\n",
              "  (0.3333333333333333, 10, 0.0, False)],\n",
              " 3: [(0.3333333333333333, 15, 1.0, True),\n",
              "  (0.3333333333333333, 10, 0.0, False),\n",
              "  (0.3333333333333333, 13, 0.0, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*yUMc-oRhXJoUBeNbKD3fcA.png)"
      ],
      "metadata": {
        "id": "ZBMWiwjebgCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ejemplo, estando en la celda 14: si elegimos la opción 1 (ir abajo), nos topamos con una valla transparente. Pero en lugar de volver a la misma celda 14, tenemos un tercio de probabilidad de lograrlo, con esa misma probabilidad (un tercio) podemos aparecer en la celda 13 o la celda 15."
      ],
      "metadata": {
        "id": "2QIBYuZdho0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Configuración del problema a resolver"
      ],
      "metadata": {
        "id": "hzfUMvLAwefX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Episodio y trayectoria\n",
        "\n",
        "$r=(s_0, a_0, r_1, s_1, a_1, r_2, s_2, ..., a_H, r_{H+1}, s_{H+1})$"
      ],
      "metadata": {
        "id": "BvwIp6ZUwiJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retorno con descuento\n",
        "\n",
        "$G_t = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + ... = \\sum_{k=t+1}^{T}r_k$\n",
        "\n",
        "$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + ... = \\sum_{k=t+1}^{T}\\gamma^{k-t-1} r_k$"
      ],
      "metadata": {
        "id": "m8FWnIuqwkuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Política"
      ],
      "metadata": {
        "id": "jRyIIfA7wnhS"
      }
    }
  ]
}