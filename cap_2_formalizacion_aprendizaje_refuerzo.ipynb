{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cap_2_formalizacion_aprendizaje_refuerzo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVtUBdv35AyY4dd+7j58HY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jazielinho/aprendizaje_refuerzo/blob/main/cap_2_formalizacion_aprendizaje_refuerzo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.1 Proceso de decisión de Markov"
      ],
      "metadata": {
        "id": "ROZBQYMkdQpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recapitulación de conceptos\n",
        "\n",
        "Un agente influye en el comportamiento observado de un entorno eligiendo acciones. El objetivo es elegir las acciones que maximicen la recompensa.\n",
        "\n",
        "El agente y el entorno interactúan en una secuencia de *time step* discretos.\n",
        "\n",
        "En el time step $t$:\n",
        "* $s$: estado del entorno ($s_{t})$\n",
        "* $a$: acción del agente ($a_{t})$\n",
        "\n",
        "La acción $a$ recibe en el siguiente time step $t + 1$:\n",
        "* $r'$ recompensa ($r_{t+1})$\n",
        "* $s'$: el entorno cambia al nuevo estado ($s_{t+1})$\n",
        "\n",
        "El conjunto del estado, acción, recompensa y nuevo estado se llama *experiencia*"
      ],
      "metadata": {
        "id": "SupT0jp5eEwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*x-Bj67wALCbWrtSNhb5h0A.png)"
      ],
      "metadata": {
        "id": "DwvJpv7WgjK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Proceso de decisión de Markov\n",
        "O *Markov Decision Process* (MDP), marco matemático para resolver algunos de los problemas de aprendizaje por refuerzo (RL)\n",
        "\n",
        "Consiste en una tupla de 5 elementos: $<S, A, R, P, γ>$\n",
        "\n",
        "* $S$: conjunto de estados.\n",
        "* $A$: conjunto de acciones.\n",
        "* $R$: función de recompensa.\n",
        "* $P$: función de transición.\n",
        "* $\\gamma$: factor de descuento."
      ],
      "metadata": {
        "id": "WhBtMjz8iegT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 Piezas de un proceso de decisión de Markov"
      ],
      "metadata": {
        "id": "rlA6aLztjBxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "entorno  = gym.make('FrozenLake-v0', is_slippery=False)"
      ],
      "metadata": {
        "id": "9THHTAWbl5Xo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Estados\n",
        "\n",
        "U *observación* es una configuración única y autónoma del problema.\n",
        "*espacio de estado*: conjunto de todos los estados posibles, puede ser infinito."
      ],
      "metadata": {
        "id": "vvrZ_OvmmKI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Espacio de estados '''\n",
        "print(f\"Espacio de estados: {entorno.observation_space}\")"
      ],
      "metadata": {
        "id": "XX133hLWeEMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794f92d3-635f-4a70-bfc5-2b13f0719861"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Espacio de estados: Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estados particulares, ejemplo, estado de partida o estados terminales. En Frozen Lake: *estado inicial* (0), 5 *estados terminales* (5, 7, 11, 12, 15)"
      ],
      "metadata": {
        "id": "mklxn6RSmpMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/1400/1*yTHeP5HSKXCpnv9wbgGIoQ.png)"
      ],
      "metadata": {
        "id": "gmzgg1ihoODr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Propiedad de Markov*: el futuro depende solo del presente y no del pasado. El estado actual que obtenemos del entorno contiene lo que se necesita para decidir el estado futuro cuando se realiza una acción.\n",
        "\n",
        "La mayoría de los métodos de RL están diseñados para aprovechar esta suposición.\n",
        "\n",
        "Ejemplo, Frozen Lake, estado actual: 2, el agente solo puede realizar la transición a 1, 2, 3, o 6, independientemente del estado anterior."
      ],
      "metadata": {
        "id": "Yn7EDaKfoWyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Acciones\n",
        "Un conjunto de acciones $A$ dependen del estado, por lo que pueden variar en cada *time step*.\n",
        "El agente elige una acción, esto influye en el entorno y el entorno cambia el estado."
      ],
      "metadata": {
        "id": "ghCD3NHCpJiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' 4 acciones '''\n",
        "''' izquierda, abajo, derecha, arriba '''\n",
        "print(f\"Espacio de acciones: {entorno.action_space}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF3W5sU0mIhS",
        "outputId": "2ffe5314-c4d6-4ef0-e7fc-bd01d6a82cd4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Espacio de acciones: Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Espacio de acción discreto: el espacio de acción consta de acciones discretas (ejemplo, Frozen Lake)\n",
        "* Espacio de acción continua: el espacio de acción compone de acciones continuas (ejemplo, conducción)"
      ],
      "metadata": {
        "id": "CCBfwMZJqFWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sOHWlpP0ppCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}